import pytest
import torch
from torch import optim

# allow importing your module
from training.train_ae import LitWrapper
from training.train_ae import main as hydra_main

# --- Fixtures --------------------------------------------------------------


@pytest.fixture
def dummy_mnist_batch():
    # mimic conftest dummy_mnist_batch
    imgs = torch.randn(4, 1, 28, 28)
    labels = torch.randint(0, 10, (4,))
    return imgs, labels


@pytest.fixture
def lit_module():
    # instantiate the LightningModule
    return LitWrapper()


# --- Tests -----------------------------------------------------------------


def test_litwrapper_training_step(lit_module, dummy_mnist_batch, monkeypatch):
    # Flattening happens inside forward, so batch is fine
    batch = dummy_mnist_batch
    # emulate batch from DataLoader: (imgs, labels)
    loss = lit_module.training_step(batch, batch_idx=0)
    assert isinstance(loss, torch.Tensor)
    # should be a single scalar
    assert loss.dim() == 0


def test_litwrapper_validation_step(lit_module, dummy_mnist_batch, caplog):
    caplog.set_level("INFO")
    # validation_step logs 'val_loss'
    lit_module.log = lambda name, value: caplog.info(f"{name}: {value.item()}")
    lit_module.validation_step(dummy_mnist_batch, batch_idx=0)
    # ensure our patched log was called
    assert any("val_loss:" in rec.message for rec in caplog.records)


def test_configure_optimizers_returns_optimizer(lit_module):
    opt = lit_module.configure_optimizers()
    assert isinstance(opt, optim.Adam)
    # check it has at least one parameter group
    assert hasattr(opt, "param_groups") and len(opt.param_groups) > 0


def test_main_entry_point_returns_exit_code_zero(monkeypatch, tmp_path):
    """
    Calling hydra_main() directly will try to parse sys.argv;
    Monkeypatch to pass a minimal config and prevent actual training.
    """

    # dummy config to bypass Hydra argument parsing
    class DummyCfg:
        datamodule = {
            "_target_": "src.data.mnist_datamodule.MNISTDataModule",
            "batch_size": 2,
            "num_workers": 0,
            "data_dir": str(tmp_path),
        }
        model = {"hidden_sizes": [16, 8], "activation": "relu", "lr": 1e-3}
        trainer = {"max_epochs": 1, "fast_dev_run": True}

    monkeypatch.setattr(hydra_main, "__wrapped__", lambda cfg: None)
    # ensure calling main() does not SystemExit(1)
    hydra_main.callback = None
    try:
        hydra_main(cfg=DummyCfg())
    except SystemExit:
        pytest.skip("Hydra entry point invoked SystemExit â€” skipping real-run test")
